{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "What happens when we hit the Inference Button (tm)? Is it gradient descent that is happening underneath the hood? What is this \"sampler\" we speak about, and what exactly is it doing?\n",
    "\n",
    "As we take a detour away from PyMC3 for a moment,\n",
    "those fundamental questions are the questions\n",
    "that we are going to go through in this chapter.\n",
    "It's my hope that you'll enjoy peeling back the covers\n",
    "on _some_ of what happens underneath the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the beginning...\n",
    "\n",
    "First off, we must remember that with Bayesian statistical inference,\n",
    "we are most concerned with computing the posterior distribution of parameters\n",
    "conditioned on data, $P(H|D)$.\n",
    "Here, $H$ refers to the parameter set and the model,\n",
    "while $D$ refers to the data.\n",
    "\n",
    "Because it is a conditional distribution,\n",
    "by invoking the rules of probability, where\n",
    "\n",
    "$$P(H,D)=P(H|D)P(D)=P(D|H)P(D)$$\n",
    "\n",
    "if you were to treat each of the $P$s as algebraic elements,\n",
    "then a simple rearrangement gives us:\n",
    "\n",
    "$$P(H|D)=\\frac{P(D|H)P(H)}{P(D)}$$\n",
    "\n",
    "This, then, is Bayes' rule as applied to joint distributions\n",
    "of data and model parameters.\n",
    "\n",
    "One hiccup shows here, though,\n",
    "in that we cannot analytically know how to calculate $P(D)$.\n",
    "The reason for this is that we don't have an analytical form\n",
    "for the probability distribution of how data could have been configured.\n",
    "In practice, we treat that as a normalizing constant,\n",
    "since philosophically, data are considered constant\n",
    "while parameters are random variables. \n",
    "Hence, our posterior distribution\n",
    "is calculated as a proportionality term:\n",
    "\n",
    "$$P(H|D) \\propto P(D|H)P(H)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at an illustration\n",
    "\n",
    "To make everything a bit more concrete, let's look at what I call\n",
    "a \"simplest complex example\",\n",
    "one that is not too hard to \"grok\" (geek slang for _understand_),\n",
    "but one that is complex enough to be interesting.\n",
    "\n",
    "We're going to inspect this particular model:\n",
    "\n",
    "\n",
    "$$\\mu_d \\sim N(\\mu=3, \\sigma=1)$$\n",
    "$$\\sigma_d \\sim Exp(\\lambda=13)$$\n",
    "$$d \\sim N(\\mu=\\mu_d, \\sigma=\\sigma_d)$$\n",
    "\n",
    "We have Gaussian-distributed data,\n",
    "where the mean of the data distribution,\n",
    "$\\mu_d$ is a Gaussian-distributed random variable\n",
    "that has a configuration that specifies our prior belief about it\n",
    "having not seen any data,\n",
    "while the variance of the data distribution,\n",
    "$\\sigma_d$ is an Exponentially-distributed random variable,\n",
    "also configured in a way that specifies our prior without having seen any data.\n",
    "\n",
    "The model's PGM looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft import PGM\n",
    "\n",
    "G = PGM()\n",
    "G.add_node(\"mu\", content=r\"$\\mu$\")\n",
    "G.add_node(\"sigma\", content=r\"$\\sigma$\", x=1)\n",
    "G.add_node(\"d\", content=\"d\", x=0.5, y=-1)\n",
    "\n",
    "G.add_edge(\"mu\", \"d\")\n",
    "G.add_edge(\"sigma\", \"d\")\n",
    "G.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Joint log-likelihood\n",
    "\n",
    "Write down the joint log-likelihood between data and the model parameters under the pre-specified priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, expon\n",
    "def log_like(mu, sigma, data):\n",
    "    mu_like = norm(loc=0, scale=3).logpdf(mu)\n",
    "    sigma_like = expon(scale=1).logpdf(sigma)\n",
    "    data_like = norm(loc=mu, scale=sigma).logpdf(data).sum() # sum is important!\n",
    "    return mu_like + sigma_like + data_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm going to give you some _actual_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = norm(-1, 1).rvs(100).reshape(-1, 1)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'd like you to propose a $\\mu$ and a $\\sigma$,\n",
    "and then evaluate their joint log-likelihood with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "log_like(-1, 1, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot how the log likelihood varies with $\\mu$ and $\\sigma$.\n",
    "This will give us a great way to visualize the posterior distribution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "mu = FloatSlider(min=-3, max=5, value=0, step=0.1)\n",
    "sigma = FloatSlider(min=0.1, max=10, value=1, step=0.1)\n",
    "\n",
    "@interact(mu=mu, sigma=sigma)\n",
    "def plot_univariate_posterior(mu, sigma):\n",
    "    # mu = -1\n",
    "    mu_range = np.linspace(-3, 5)\n",
    "    # sigma = 1\n",
    "    sigma_range = np.linspace(0.1, 10)\n",
    "    ll_sigma = log_like(mu, sigma_range, data)\n",
    "    ll_mu = log_like(mu_range, sigma, data)\n",
    "    fig, ax = plt.subplots(figsize=(8,4), nrows=1, ncols=2)\n",
    "    ax[0].plot(sigma_range, ll_sigma)\n",
    "    ax[1].plot(mu_range, ll_mu)\n",
    "    sns.despine()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "lldf = pd.DataFrame(points)\n",
    "\n",
    "sns.scatterplot(data=lldf, x=\"mu\", y=\"sigma\", hue=\"ll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = np.linspace(0, 6, 100)\n",
    "sigmas = np.linspace(0.1, 20, 100)\n",
    "\n",
    "MUS, SIGMAS = np.meshgrid(mus, sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ll = log_like(MUS, SIGMAS, data)\n",
    "plt.contour(MUS, SIGMAS, ll, levels=100)\n",
    "# plt.colorbar()\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
