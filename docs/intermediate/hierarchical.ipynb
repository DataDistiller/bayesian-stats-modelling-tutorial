{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter, you saw how to build what we might call \"multiple\" estimation models.\n",
    "In the example we've been working through, we have gone from\n",
    "estimationg $p$ for a single store to estimating $p$ for 1400+ stores.\n",
    "\n",
    "Something that you might have noticed is that\n",
    "some of the stores had really wide posterior distribution estimates.\n",
    "Depending on your beliefs about the world,\n",
    "this might be considered quite dissatisfying.\n",
    "We might ask, for example, are there really no pieces of information in the data\n",
    "that we might leverage to make more informed inferences\n",
    "about the true like-ability of an ice cream shop?\n",
    "\n",
    "Well, if you remember in the dataset,\n",
    "there was another column that we did not use, `owner_idx`.\n",
    "Let's see if that column might be of any use for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.data import load_ice_cream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_ice_cream()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janitor\n",
    "import numpy as np\n",
    "\n",
    "naive_p = (\n",
    "    data\n",
    "    .join_apply(  # calculate naive_p\n",
    "        lambda x: \n",
    "            x[\"num_favs\"] / x[\"num_customers\"] \n",
    "            if x[\"num_favs\"] > 0 \n",
    "            else np.nan, \n",
    "        new_column_name=\"naive_p\"\n",
    "    )\n",
    ")\n",
    "\n",
    "(\n",
    "    naive_p\n",
    "    .groupby(\"owner_idx\")\n",
    "    .agg({\"naive_p\": [\"mean\", \"count\", \"std\"]})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.swarmplot(data=naive_p, y=\"naive_p\", x=\"owner_idx\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the visualization, it seems to me that that each of the owners might have a \"characteristic\" $p$,\n",
    "and that each of the owners might also have its own characteristic degree of variability amongst stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generating Process\n",
    "\n",
    "If we were to re-think our data generating process, we might suggest a slightly modified story.\n",
    "\n",
    "Previously, we thought of our data generating process as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_tutorial.solutions.estimation import ice_cream_n_group_pgm, ice_cream_one_group_pgm\n",
    "\n",
    "ice_cream_n_group_pgm()\n",
    "# pgm = ice_cream_one_group_pgm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each shop has its own $p$, and that generates its own \"likes\".\n",
    "Each $p_i$ is drawn from its own Beta distribution,\n",
    "configured with a common $\\alpha$ and $\\beta$.\n",
    "\n",
    "What if we tried to capture the idea that each shop draws its $p$ from its owners?\n",
    "Here's where the notion of hierarchical models comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Models\n",
    "\n",
    "In a \"hierarchical\" version of the ice cream shop model,\n",
    "we try to express the idea that not only does each shop have its own $p$,\n",
    "it's $p$ is somehow conditionally dependent on its owner's $p$.\n",
    "\n",
    "More generally, with a hierarchical model,\n",
    "we impose the assumption\n",
    "that each sample draws its key parameters from a \"population\" distribution.\n",
    "Underlying this assumption is the idea\n",
    "that \"things from the same group should be put together\".\n",
    "\n",
    "If we ignored (for a moment) the \"fixed\" variables,\n",
    "then the hierarchical model would look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft import PGM, Node\n",
    "\n",
    "G = PGM()\n",
    "G.add_node(\"p_shop\", content=r\"$p_{j, i}$\", x=1, y=2, scale=1.2)\n",
    "G.add_node(\"likes\", content=\"$l_{j, i}$\", x=1, y=1, scale=1.2, observed=True)\n",
    "G.add_node(\"p_owner\", content=r\"$p_{j}$\", x=1, y=3, scale=1.2)\n",
    "G.add_node(\"p_pop\", content=r\"$p$\", x=1, y=4, scale=1.2)\n",
    "\n",
    "G.add_edge(\"p_pop\", \"p_owner\")\n",
    "G.add_edge(\"p_owner\", \"p_shop\")\n",
    "G.add_edge(\"p_shop\", \"likes\")\n",
    "\n",
    "G.add_plate(plate=[0.3, 0.3, 1.5, 2.2], label=r\"shop $i$\")\n",
    "G.add_plate(plate=[0, -0.1, 2.1, 3.6], label=r\"owner $j$\")\n",
    "\n",
    "G.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are expressing the idea that each shop $i$ draws its $p_{j, i}$ from its the $p_{j}$ associated with its owner $j$,\n",
    "and that its owner $p_{j}$ draws from a population $p$ distribution governing all owners.\n",
    "\n",
    "In theory, this is really cool.\n",
    "But implementing this is kind of difficult,\n",
    "if we think more closely about the structure we've used thus far.\n",
    "With Beta distributions as priors,\n",
    "we might end up with a very convoluted structure instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = PGM()\n",
    "G.add_node(\"likes\", content=\"$l_{j, i}$\", x=1, y=1, scale=1.2, observed=True)\n",
    "G.add_node(\"p_shop\", content=\"$p_{j, i}$\", x=1, y=2, scale=1.2)\n",
    "G.add_node(\"alpha_owner\", content=r\"$\\alpha_{j}$\", x=0, y=3, scale=1.2)\n",
    "G.add_node(\"beta_owner\", content=r\"$\\beta_{j}$\", x=2, y=3, scale=1.2)\n",
    "G.add_node(\"lambda_a_pop\", content=r\"$\\lambda_{\\alpha}$\", x=0, y=4, scale=1.2)\n",
    "G.add_node(\"lambda_b_pop\", content=r\"$\\lambda_{\\beta}$\", x=2, y=4, scale=1.2)\n",
    "G.add_node(\"tau_lambda_a\", content=r\"$\\tau_{\\lambda_{\\alpha}}$\", x=0, y=5, fixed=True)\n",
    "G.add_node(\"tau_lambda_b\", content=r\"$\\tau_{\\lambda_{\\beta}}$\", x=2, y=5, fixed=True)\n",
    "\n",
    "G.add_edge(\"alpha_owner\", \"p_shop\")\n",
    "G.add_edge(\"beta_owner\", \"p_shop\")\n",
    "G.add_edge(\"p_shop\", \"likes\")\n",
    "G.add_edge(\"lambda_a_pop\", \"alpha_owner\")\n",
    "G.add_edge(\"lambda_b_pop\", \"beta_owner\")\n",
    "G.add_edge(\"tau_lambda_a\", \"lambda_a_pop\")\n",
    "G.add_edge(\"tau_lambda_b\", \"lambda_b_pop\")\n",
    "\n",
    "G.add_plate(plate=[0.5, 0.2, 1, 2.3], label=r\"shop $i$\")\n",
    "G.add_plate(plate=[-0.5, 0, 3, 3.5], label=r\"owner $j$\")\n",
    "G.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure how you feel looking at that PGM diagram,\n",
    "but at least to me, it looks convoluted.\n",
    "I'd find it a hassle to implement.\n",
    "Also, I wouldn't be able to bake in interpretability into the model\n",
    "by directly mapping key parameter values to quantities of interest.\n",
    "\n",
    "The key problem here is that of _parameterization_.\n",
    "By _directly_ modelling $p$ with a Beta distribution,\n",
    "we are forced to place priors on the $\\alpha$ and $\\beta$ parameters\n",
    "of the Beta distribution.\n",
    "That immediately precludes us\n",
    "from being able to model the \"central tendencies\"\n",
    "of owner-level shop ratings.\n",
    "\n",
    "To get around this, I'm going to introduce you to this idea\n",
    "of transforming a random variable,\n",
    "which is immensely helpful in modelling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations of random variables\n",
    "\n",
    "In our application,\n",
    "being able to model directly the \"central tendency\" of the $p$,\n",
    "for each shop and owner, matters a lot.\n",
    "\n",
    "A Beta distribution parameterization does not allow us\n",
    "to model $p$ with \"central tendencies\" directly.\n",
    "\n",
    "On the other hand, if we were to \"transform\" the random variable $p$,\n",
    "which has bounded support between 0 and 1,\n",
    "into a regime that did not have a bounded support,\n",
    "we could conveniently use Gaussian distributions,\n",
    "which have central tendency parameters that we can model\n",
    "using random variables directly.\n",
    "\n",
    "### Logit Transform\n",
    "\n",
    "One such transformation for a random variable that is bounded\n",
    "is the **logit transform**.\n",
    "In math form, given a random variable $p$ that is bounded in the $[0, 1]$ interval,\n",
    "the logit transformation like this:\n",
    "\n",
    "$$f(p) = \\log(\\frac{p}{1-p})$$\n",
    "\n",
    "To help you understand a bit of the behaviour of the logit function, here it is plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logit\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "p = np.linspace(0, 1, 1000)\n",
    "logit_p = logit(p)\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "plt.plot(p, logit_p)\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"logit(p)\")\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the logit transformation function maps values on the interval between 0 and 1\n",
    "onto an interval that is in the interval $(-\\infty, \\infty)$.\n",
    "And since the transformed random variable has infinite support,\n",
    "we can use a distribution that has infinite support to model it.\n",
    "\n",
    "Remember also that we desired a way to model the central tendencies of our random variables,\n",
    "and so a highly natural choice here is to use the Gaussian distribution,\n",
    "which has a central tendency parameter $\\mu$.\n",
    "As such, we can instantiate a random variable for the _logit transformed version of $p$_,\n",
    "and then use the inverse logit transformation to take it back to bounded $(0, 1)$ space,\n",
    "which we can then use for our Binomial likelihood function for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "from scipy.stats import norm\n",
    "from scipy.special import expit\n",
    "\n",
    "mu = FloatSlider(value=0, min=-3, max=3, step=0.1)\n",
    "sigma = FloatSlider(value=1, min=0, max=5, step=0.1)\n",
    "\n",
    "@interact(mu=mu, sigma=sigma)\n",
    "def plot_mu_p(mu, sigma):\n",
    "    xs = np.linspace(mu - sigma * 4, mu + sigma * 4, 1000)\n",
    "    ys = norm(loc=mu, scale=sigma).pdf(xs)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), sharey=True)\n",
    "    ax[0].plot(xs, ys)\n",
    "    ax[0].set_xlabel(r\"$\\mu$\")\n",
    "    ax[0].set_ylabel(\"PDF\")\n",
    "    ax[1].plot(expit(xs), ys)\n",
    "    ax[1].set_xlim(0, 1)\n",
    "    ax[1].set_xlabel(r\"p = invlogit($\\mu$)\")\n",
    "    sns.despine()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical model for \"indie\" stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "n_shops = len(data[\"owner_idx\"].unique())\n",
    "with pm.Model() as model:\n",
    "    logit_p_overall = pm.Normal(\n",
    "        \"logit_p_overall\",\n",
    "        mu=0,\n",
    "        sigma=1\n",
    "    )\n",
    "    logit_p_owner_mean = pm.Normal(\n",
    "        \"logit_p_owner_mean\",\n",
    "        mu=logit_p_overall,\n",
    "        sigma=1,\n",
    "        shape=(n_shops,)\n",
    "    )\n",
    "    logit_p_owner_scale = pm.Exponential(\n",
    "        \"logit_p_owner_scale\",\n",
    "        lam=1/5.,\n",
    "        shape=(n_shops,)\n",
    "    )\n",
    "    logit_p_shop = pm.Normal(\n",
    "        \"logit_p_shop\",\n",
    "        mu=logit_p_owner_mean[data[\"owner_idx\"]],\n",
    "        sigma=logit_p_owner_scale[data[\"owner_idx\"]],\n",
    "        shape=(len(data),)\n",
    "    )\n",
    "    \n",
    "    p_overall = pm.Deterministic(\"p_overall\", pm.invlogit(logit_p_overall))\n",
    "    p_shop = pm.Deterministic(\"p_shop\", pm.invlogit(logit_p_shop))\n",
    "    p_owner = pm.Deterministic(\"p_owner\", pm.invlogit(logit_p_owner_mean))\n",
    "    like = pm.Binomial(\"like\", n=data[\"num_customers\"], p=p_shop, observed=data[\"num_favs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "with model:\n",
    "    trace = az.from_pymc3(trace, coords={\"p_shop_dim_0\": data[\"shopname\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(trace, var_names=[\"p_owner\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"p_owner\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace, var_names=[\"logit_p_owner_scale\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import janitor\n",
    "\n",
    "quantiles = (\n",
    "    trace.posterior\n",
    "    .stack(draws=(\"chain\", \"draw\"))\n",
    "    [\"p_owner\"]\n",
    "    .quantile(q=[0.03, 0.5, 0.97], dim=(\"draws\"))\n",
    ")\n",
    "quantiles = (\n",
    "    quantiles\n",
    "    .to_dataframe()\n",
    "    .reset_index()\n",
    "    .pivot_table(columns=\"quantile\", index=\"p_owner_dim_0\", values=\"p_owner\")\n",
    "    .join_apply(lambda x: x[0.97] - x[0.03], \"width\")\n",
    ")\n",
    "quantiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian-modelling-tutorial",
   "language": "python",
   "name": "bayesian-modelling-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
